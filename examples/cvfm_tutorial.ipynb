{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Variable Flow Matching: Extending Flow Matching to learn amortized conditional mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The original formulations of Flow Matching ([Lipman et al. (2023)](https://arxiv.org/abs/2210.02747)), including Stochastic Interpolants ([Albergo et al. (2023)](https://arxiv.org/abs/2209.15571)) and Rectified Flows ([Liu et al. (2023)](https://arxiv.org/abs/2209.03003)), have demonstrated a remarkable ability to train continuous normalizing flows (CNF) in a simulation-free manner. The core insight of these approaches lies in their ability to efficiently learn the dynamics transforming a base density to the data distribution, following an probability path specified by an ordinary differential equation (ODE). \n",
    "\n",
    "This notebook details our approach towards generalizing this efficient objective towards accepting conditional variables with continuous support.\n",
    "\n",
    "## Flow Matching\n",
    "\n",
    "CNFs define a contiuous map, taking data samples from a base distribution $x_0 \\sim p(x_0)$ to a target distribution $x_1 \\sim p(x_1)$ by learning a vector field $u: [0,1] \\times \\mathbb{R}^N \\rightarrow \\mathbb{R}^N$, defining a flow $\\phi [0,1] \\times \\mathbb{R}^N \\rightarrow \\mathbb{R}^N$ according to the corresponding ordinary differential equation (ODE)\n",
    "\n",
    "$$ \\frac{d}{dt} \\phi_t(x) = u_t(\\phi_t(x)) ,\\;\\;\\; \\phi_0(x) = x. $$\n",
    "\n",
    "This flow defines a push-forward operation\n",
    " \n",
    "$$ p_t = \\left[\\phi_{t}\\right]_{\\#} p_{0} $$\n",
    "\n",
    "transforming our base distribution $p_0$ at $t=0$ to  $p_1$ at $t=1$.\n",
    "\n",
    "Not all vector fields satisfy this operation, specifically, only vector fields satisfying the continuity equation can be shown to generate the associated probability path $p_t$\n",
    "\n",
    "$$ \\frac{d}{dt} p_t(x) + \\nabla_x (p_t(x)u_t(x)) = 0 $$\n",
    "\n",
    "### Learning Objective\n",
    "\n",
    "Flow matching circumvents regressing against the intractable marginal vector field $u_t(x)$ by introducing a conditional objective\n",
    "\n",
    "$$ \\mathcal{L}_{\\textrm{CFM}}(\\theta) = \\mathbb{E}_{t, q(z), p_t(x|z)} \\lVert v_\\theta(x, t) - u_t(x | z) \\rVert ^2 $$\n",
    "\n",
    "where $q(z)$ defines the empirical distribution over observations $z = \\{x_0, x_1\\}$, $p_t(x|z)$ the conditional probability path between observations, and $u_t(x | z)$ the associated conditional vector field.\n",
    "\n",
    "Interestingly, this way of combining conditional vector fields and their generated conditional probability paths can be shown to generate the correct marginal distributions. In particular, when Gaussian paths are taken, $p_t(x|z)=\\mathcal{N}(x|\\mu_t(z),\\sigma_t^2(z))$ of $\\phi_{t,z}(x) = \\mu_t(z) + \\sigma_t(z)x$ the unique conditional vector field $u_t(x|z)$ can be solved in closed form\n",
    "\n",
    "$$ u_t(x|z) = \\frac{\\sigma'_t}{\\sigma_t}(x - \\mu_t) + \\mu'_t. $$\n",
    "\n",
    "In the case where $\\mu_t(z) = t x_1 + (1 - t)x_0$ with constant $\\sigma$ across time, the conditional vector field becomes $u_t(x|z) = x_1 - x_0$.\n",
    "\n",
    "\n",
    "## Conditional Variable Flow Matching\n",
    "\n",
    "A core limitation of this approach lies in its ability to only produce a singular probability path morphing an unconditional distribution $p_0 \\rightarrow p_1$. Conditional Variable Flow Matching (CVFM) aims to directly address this limitation by producing an objective capable of learning conditional probability paths across the conditional data mainfold. \n",
    "\n",
    "\n",
    "The original marginalization motivating the flow matching objective can be extended to construct a probability flow across both $x$ and a conditioning variable $y$ as\n",
    "\n",
    "$$  p_t(x | y) = \\int p_t(x | y, z, w)q(z, w)dzdw $$\n",
    "\n",
    "where $q(z,w)$ now denotes the empirical distribution over $z = (x_0, x_1)$ and $w = (y_0, y_1)$. We make the further assumption that the conditional joint probability path decomposes as $p_t(x, y| z, w) = p_t(x| z)p_t(y| w)$, resulting in two simultaneous conditional flows.\n",
    "\n",
    "We can also extend this line of thought towards defining a marginal conditional vector field, through marginalizing over vector fields conditioned on observations $z$ and $w$ as\n",
    "\n",
    "$$ u_t(x| y) = \\mathbb{E}_{q(z, w)} \\frac{u_t(x| z) p_t(x| z) p_t(y|w)}{p_t(x, y)} $$\n",
    "\n",
    "where $u_t(x| z): \\mathbb{R}^N  \\rightarrow \\mathbb{R}^N$ is a conditional vector field generating $p_t(x| z)$ from $p_0(x| z)$, without any explicit dependence upon the conditional distribution over our conditional variable, $y$.\n",
    "\n",
    "$$ \\mathcal{L}_{\\textrm{CVFM}}(\\theta) = \\mathbb{E}_{t, q(z, w), p_t(x | z) p_t(y| w)} \\lVert v_\\theta(x, y, t) - u_t(x | z) \\rVert^2 $$\n",
    "\n",
    "### Conditional Optimal Transport\n",
    "\n",
    "CVFM requires an optimal transport (OT) coupling in the empirical distribution $q(z, w)$, specifically across $y$ in order to satisfy the continuity equation. In other words, the mapping $q(y_0)$ to $q(y_1)$ must follow the coupling $\\pi(y_0,y_1)$.\n",
    "\n",
    "We would like to search for an OT map predominantly permitting movement across $\\mathcal{X}$ and not $\\mathcal{Y}$. In a very practical sense, given the continuous support of $\\mathcal{Y}$, such a constraint would not be feasible within a finite number of samples. Instead, we moderate this requirement in the form of the proposed continuous non-negative cost function\n",
    "\n",
    "$$ c((x_i,y_i),(x_j,y_j)) = \\lVert x_i - x_j \\rVert_p + \\eta \\lVert y_i - y_j \\rVert_p $$\n",
    "\n",
    "where $\\eta > 0$ is a parameter governing the tolerance of transport permissible in $\\mathcal{Y}$\n",
    "\n",
    "To further facilitate conditional OT, a stationary kernel with support across the conditioning variable is introduced $\\alpha(w) = \\exp(-(y_0 - y_1)/{2\\sigma_y^2})$, modulating the objective as\n",
    "\n",
    "$$ \\mathcal{L}_{\\textrm{CVFM}}(\\theta) = \\mathbb{E}_{t, q(z, w), p_t(x | z) p_t(y| w)} \\alpha(w) \\lVert v_\\theta(x, y, t) - u_t(x | z) \\rVert^2 $$\n",
    "\n",
    "The benefits of $\\alpha(w)$ will become increasingly apparant in the 2D example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import ot as pot\n",
    "import torch\n",
    "import torchdyn\n",
    "import torchsde\n",
    "from torchdyn.core import NeuralODE\n",
    "from torchdyn.datasets import generate_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_moons(n, theta):\n",
    "    theta = np.radians(theta)\n",
    "    rot_mat = torch.tensor([[np.cos(theta), -np.sin(theta)],\n",
    "                             [np.sin(theta),  np.cos(theta)]],dtype=torch.float32)\n",
    "    \n",
    "    x0, y0 = generate_moons(n, noise=0.2)\n",
    "    x0 = 3*(x0 - 0.5)\n",
    "    x0 = torch.matmul(x0, rot_mat)\n",
    "    return x0, y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, dim, cdim, w=64):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.cdim = cdim\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(dim + cdim + 1, w),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(w, w),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(w, w),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(w, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y, t):\n",
    "        in_x = torch.cat([x, y, t], dim=-1)\n",
    "        out = self.net(in_x)\n",
    "        return out\n",
    "    \n",
    "class torchdyn_wrapper(torch.nn.Module):\n",
    "    def __init__(self, model, y):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.y = y\n",
    "        \n",
    "    def forward(self, t, x, args=None):  \n",
    "        return self.model(x, self.y, t.repeat(x.shape[0])[:, None]) \n",
    "\n",
    "def plot_trajectories(traj, y):\n",
    "    n = 2000\n",
    "    y = y[:n].astype('int')\n",
    "    \n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    norm = plt.Normalize(vmin=y.min(), vmax=y.max())\n",
    "    colors = cmap(norm(y)) \n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(traj[0, :n, 0], traj[0, :n, 1], s=10, alpha=0.8, c=\"black\",zorder=5)\n",
    "    for i in range(n):\n",
    "        plt.plot(traj[:, i, 0], traj[:, i, 1], color=colors[i], alpha=0.1,zorder=0)\n",
    "    plt.scatter(traj[-1, :n, 0], traj[-1, :n, 1], s=4, alpha=1, c=\"maroon\",zorder=10)\n",
    "    \n",
    "    plt.legend([r\"$p_0$\", r\"$x_{t} \\vert x_{0}$\", r\"$p_1$\"])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sigma = 0.1\n",
    "sigmay = 0.5\n",
    "oty = 5\n",
    "dim = 2\n",
    "cdim = 1\n",
    "batch_size = 256\n",
    "\n",
    "model = MLP(dim=dim, cdim=cdim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000: loss 0.079\n",
      "4000: loss 0.076\n",
      "6000: loss 0.059\n",
      "8000: loss 0.081\n",
      "10000: loss 0.058\n"
     ]
    }
   ],
   "source": [
    "a, b = pot.unif(batch_size), pot.unif(batch_size)\n",
    "for k in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    t = torch.randn(batch_size, 1).to(device)\n",
    "    t = torch.nn.Sigmoid()(t)\n",
    "    x0, y0 = sample_moons(batch_size, 0)\n",
    "    x1, y1 = sample_moons(batch_size, 270)\n",
    "    y0 = (x0[:,0] - 10)*(y0) + (1 - y0)*(x0[:,0] + 10)\n",
    "        \n",
    "    y1 = y0\n",
    "    y0 = y0.unsqueeze(-1)\n",
    "    y1 = y1.unsqueeze(-1)\n",
    "    \n",
    "    # Randomly shuffle to account for torchdyn ordered sampling\n",
    "    ind0 = torch.randperm(x0.size()[0])\n",
    "    ind1 = torch.randperm(x1.size()[0])\n",
    "    x0, y0 = x0[ind0].to(device), y0[ind0].to(device)\n",
    "    x1, y1 = x1[ind1].to(device), y1[ind1].to(device)  \n",
    "    \n",
    "    # Resample xy0, xy1 according to transport matrix\n",
    "    M = torch.cdist(x0, x1) ** 2 + oty*torch.cdist(y0, y1) ** 2\n",
    "    M = M / M.max()\n",
    "    pi = pot.emd(a, b, M.detach().cpu().numpy())\n",
    "    p = pi.flatten()\n",
    "    p = p / p.sum()\n",
    "    choices = np.random.choice(pi.shape[0] * pi.shape[1], p=p, size=batch_size, replace=False)\n",
    "    i, j = np.divmod(choices, pi.shape[1])   \n",
    "    x0 = x0[i]\n",
    "    x1 = x1[j]\n",
    "    y0 = y0[i] \n",
    "    y1 = y1[j]\n",
    "\n",
    "    # calculate regression loss\n",
    "    mu_t = x0 * (1 - t) + x1 * t\n",
    "    muy_t = y0 * (1 - t) + y1 * t\n",
    "    \n",
    "    x = mu_t + sigma * torch.randn_like(x0).to(device)\n",
    "    y = muy_t + sigmay * torch.randn_like(y0).to(device)\n",
    "    \n",
    "    ut = x1 - x0\n",
    "    vt = model(x, y, t)  \n",
    "\n",
    "    # SE kernel in conditioning variable\n",
    "    yerr = torch.exp(-0.5 * ((y1 - y0) / sigmay) ** 2)\n",
    "\n",
    "    loss = torch.mean(yerr * (vt - ut) ** 2) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (k + 1) % 2000 == 0:\n",
    "        print(f\"{k+1}: loss {loss.item():0.3f}\")\n",
    "\n",
    "        x0, y0 = sample_moons(2048, 0)\n",
    "        x0, y0 = x0.to(device), y0.to(device)\n",
    "        y0 = (x0[:,0] - 10)*(y0) + (1 - y0)*(x0[:,0] + 10)\n",
    "        node = NeuralODE(\n",
    "            torchdyn_wrapper(model, y0.unsqueeze(-1)), solver=\"dopri5\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            traj = node.trajectory(\n",
    "                x0,\n",
    "                t_span=torch.linspace(0, 1, 100),\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
